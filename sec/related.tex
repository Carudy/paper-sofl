Homomorphic encryption based solutions are intuitively effective to solve aggregation problems, and there are many researchers tried to reduce the overhead caused by HE. Stephen et al.~\cite{abs-1711-10677} designed an additively homomorphic encryption method for federated learning in 2017. However, the encryption method is complicated that it does not reach high efficiency. The state-of-the-art HE method was proposed by Zhang and Li~\cite{BatchCrypt}. It reduced the encryption overhead greatly at the cost of trivial loss of accuracy. However, it still costs much time on computation compared to MPC based and DP based methods.

DP based solutions have better performance than HE based solutions, while Wei et al.\cite{DPAnalysis} has indicated that there is a tradeoff between the performance and security levels, which means it needs numerous adjustments to adopt DP methods efficiently. Robin et al~\cite{geyer2017differentially}. tested DP in FL in 2017 and the result showed that DP's influence on accuracy is untrivial. Bayesian differential privacy~\cite{Bayesian} was proposed in 2019. It considered the probability and distribution of data and is effective for machine learning models whose data are often restricted to a particular type. While it may be inefficient in vertical federated learning situations. Whereas all these methods did not prove DP does not impact the accuracy in other more complicated and large-scale models, which means it is different to find a universal DP method for all machine learning models.

MPC based methods have the least computation cost but require more communications. A typical MPC federated learning model is implemented by Google~\cite{Practical}. It requires pairwise key exchange among all clients, which results in enormous overhead on communication. It also provides robustness by means of mask mechanism with some random numbers. However, these mechanisms bring about more overhead. Other researches present hybrid methods~\cite{Hybrid,HybridAlpha}. These hybrid methods combine MPC with either HE or DP and make tradeoff on computation and communication.

Federated learning is more and more practical nowadays and there are already many FL platforms: FATE~\cite{fate} is an open-source federated learning project proposed by Webankâ€™s AI Department. It adopts both MPC and HE to implement secure aggregation, while it is still absorbing state-of-the-art methods for privacy-preserving. Pysyft~\cite{pysyft} is another open-source federated learning framework presented by OpenMined. It is based on Pytorch and offers HE, DP, and MPC as alternative methods to realize privacy-preserving. However, the cost of time of Pysyft is dozens of times than pure Pytorch, which indicates that privacy-preserving methods of current federated learning platforms need to be improved.
